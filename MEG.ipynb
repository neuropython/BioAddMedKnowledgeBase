{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neuropython/BioAddMedKnowledgeBase/blob/master/MEG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97b3da31-8b31-4bc7-bc28-b3903e21616c"
      },
      "source": [
        "# Magnetoencephalography data analysis"
      ],
      "id": "97b3da31-8b31-4bc7-bc28-b3903e21616c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f88c5278-88a0-40ba-ad1a-58172cb04fae"
      },
      "source": [
        "## Dataset description"
      ],
      "id": "f88c5278-88a0-40ba-ad1a-58172cb04fae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df31d3ef-9eef-4d5f-9dd3-94c8c7f7cd65"
      },
      "source": [
        "In this exercise you will be using the ds000117 dataset  entitled  [Integrated Neuroimaging Dataset from Perceptual Tasks](https://www.openfmri.org/dataset/ds000117/) from the OpenfMRI database. This dataset is a comprehensive collection of neuroimaging data obtained from nineteen healthy volunteers, focusing on both functional and structural brain modulations. Functional data include results from electroencephalography (EEG), magnetoencephalography (MEG), and functional magnetic resonance imaging (fMRI). The experiment centred around the performance of a simple perceptual task, where participants were asked to analyse images containing familiar, unfamiliar, and scrambled faces.\n",
        "\n"
      ],
      "id": "df31d3ef-9eef-4d5f-9dd3-94c8c7f7cd65"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5117e5-475f-4531-b1ad-65978f1425ef"
      },
      "source": [
        "## Library description"
      ],
      "id": "0c5117e5-475f-4531-b1ad-65978f1425ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bea24aa3-0b2d-4d2c-8b82-866a40fad699"
      },
      "source": [
        "\n",
        "\n",
        "In this task you will try to analyse MEG data using python library called [MNE](https://mne.tools/stable/index.html). MEG is a non-invasive neuroimaging technique that captures the magnetic fields generated by neural activity. MEG data acquired during experiments are commonly stored in the Elekta/Neuromag Raw Data (.fif) format. This kind of format is capable of storing a variety of data types associated with MEG recordings, including raw sensor data, events, and additional metadata. MEG data is inherently three-dimensional: each sensor (channel) captures magnetic signals over time."
      ],
      "id": "bea24aa3-0b2d-4d2c-8b82-866a40fad699"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea524e8d-2cd1-4eaf-8c57-7a896db857c8"
      },
      "source": [
        "# Let's start!"
      ],
      "id": "ea524e8d-2cd1-4eaf-8c57-7a896db857c8"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "97d3367b-014e-4bf1-9b75-f35667d0b08c",
        "outputId": "d309a7f6-99cc-49bf-9d7e-7d8492473f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mne'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6003eb9cc087>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mne'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "id": "97d3367b-014e-4bf1-9b75-f35667d0b08c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "569d137b-e9bf-4023-809c-a31d24d41fc6"
      },
      "source": [
        "# Load the MEG data"
      ],
      "id": "569d137b-e9bf-4023-809c-a31d24d41fc6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19198128-fd2e-41ad-ae42-dc55c722fec2"
      },
      "source": [
        "Load the MEG data sub-01_ses-meg_task-facerecognition_run-01_meg.fif using MNE. Once loaded, extract crucial information, including the sampling frequency, the timestamp of the last recorded sample, the names of the first five channels, and the total number of channels and timesteps."
      ],
      "id": "19198128-fd2e-41ad-ae42-dc55c722fec2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61343c3c-1714-413e-9da8-1deef6d2b424"
      },
      "outputs": [],
      "source": [
        "raw = mne.io.read_raw_fif('sub-01/ses-meg/meg/sub-01_ses-meg_task-facerecognition_run-01_meg.fif', preload=True)\n"
      ],
      "id": "61343c3c-1714-413e-9da8-1deef6d2b424"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c5a60c3-bdf4-4328-9839-42962c553f11"
      },
      "source": [
        "Now our task is to extract some information about the experiment, let look into the data..."
      ],
      "id": "7c5a60c3-bdf4-4328-9839-42962c553f11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6463a172-4a78-4ef0-a6a0-4a44e5c53013"
      },
      "outputs": [],
      "source": [
        "raw.info"
      ],
      "id": "6463a172-4a78-4ef0-a6a0-4a44e5c53013"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02edf89b-8b4b-419e-bfad-fb96855838d5"
      },
      "outputs": [],
      "source": [
        "number_of_time_samples = raw.n_times\n",
        "time_sec = raw.times\n",
        "channels_names = raw.ch_names[:5]\n",
        "number_of_channels = raw.info['nchan']\n",
        "sampling_frequency = raw.info['sfreq']\n",
        "last_sample_timestamp = raw.times[-1]\n",
        "\n",
        "print(\"Sampling frequency: {} Hz\".format(sampling_frequency))\n",
        "print(\"Timestamp of the last recorded sample: {:.2f} seconds\".format(last_sample_timestamp))\n",
        "print(\"Names of the first five channels: {}\".format(\", \".join(channels_names)))\n",
        "print(\"Total number of channels: {}\".format(number_of_channels))\n",
        "print(\"Total number of timesteps: {}\".format(number_of_time_samples))\n"
      ],
      "id": "02edf89b-8b4b-419e-bfad-fb96855838d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d458dc1c-e1eb-485f-830a-f6018aa61e20"
      },
      "source": [
        "# Filtering of the signal"
      ],
      "id": "d458dc1c-e1eb-485f-830a-f6018aa61e20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6a16d0e-8667-4180-ae66-d549f91aac3c"
      },
      "source": [
        "Now we will filter a given signal using two different frequency filters (0.1 Hz and 40 Hz) and then visualise the signal before and after. Firstly let's check how the example signal looks like for different channels:"
      ],
      "id": "d6a16d0e-8667-4180-ae66-d549f91aac3c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "442cbde0-5598-4c38-863f-f565af39f211"
      },
      "outputs": [],
      "source": [
        "raw.plot()"
      ],
      "id": "442cbde0-5598-4c38-863f-f565af39f211"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c4865f7-6590-403d-ab87-f9a8ce6f0f7d"
      },
      "source": [
        "Each channel contains a continuous signal from 0 to approximately 500 s. Each of these signals contains responses to different stimuli given at different intervals to the subject. Let's move to step of filtering the the data:"
      ],
      "id": "5c4865f7-6590-403d-ab87-f9a8ce6f0f7d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "997884c7-5ecc-4e59-9fee-9a0df9319f47"
      },
      "outputs": [],
      "source": [
        "raw.filter(l_freq=0.1, h_freq=None)"
      ],
      "id": "997884c7-5ecc-4e59-9fee-9a0df9319f47"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d9f8738-63cf-483e-a802-1af23bc03f89"
      },
      "outputs": [],
      "source": [
        "raw.plot()"
      ],
      "id": "6d9f8738-63cf-483e-a802-1af23bc03f89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d641fce5-b2ab-418f-8d89-386457b79f2f"
      },
      "source": [
        "The signal is quite noisy, but let's apply high pass filtering and see."
      ],
      "id": "d641fce5-b2ab-418f-8d89-386457b79f2f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "880a98f7-93f9-4f4b-a8cc-8d3a1b08df1f"
      },
      "outputs": [],
      "source": [
        "raw.filter(l_freq=None, h_freq=40)"
      ],
      "id": "880a98f7-93f9-4f4b-a8cc-8d3a1b08df1f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b250b1ce-0504-4c33-8ea1-c2430e4f1bb5"
      },
      "outputs": [],
      "source": [
        "raw.plot()"
      ],
      "id": "b250b1ce-0504-4c33-8ea1-c2430e4f1bb5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79bd7e89-25d9-4618-86f6-c762a20dab20"
      },
      "source": [
        "As you probobaly see we used two kinds of filters:\n",
        "\n",
        "1. **Low-pass Filtering:**\n",
        "   - *Purpose:* Low-pass filters allow signals with frequencies below a certain cutoff frequency to pass through, attenuating frequencies above that threshold.\n",
        "   - *Use Cases:*\n",
        "     - *Remove High-Frequency Noise:* High-frequency noise, such as muscle artifacts or environmental interference, can be effectively reduced by applying a low-pass filter. This is important for extracting meaningful information from the lower-frequency neural signals.\n",
        "     - *Focus on Slow Components:* In MEG signals, neural activity often occurs at lower frequencies. Applying a low-pass filter can help emphasize these slower components, making it easier to analyze and interpret neural patterns.\n",
        "\n",
        "2. **High-pass Filtering:**\n",
        "   - *Purpose:* High-pass filters allow signals with frequencies above a certain cutoff frequency to pass through, attenuating frequencies below that threshold.\n",
        "   - *Use Cases:*\n",
        "     - *Remove Low-Frequency Drift:* MEG signals may contain slow baseline drift or low-frequency artifacts. Applying a high-pass filter helps eliminate these low-frequency components, revealing more dynamic and relevant neural activity.\n",
        "     - *Highlight Fast Changes:* High-pass filtering is useful when researchers are interested in rapid changes in neural activity, such as those associated with cognitive processes or transient responses.\n",
        "\n",
        "**Overall:**\n",
        "   - *Noise Reduction:* Both low-pass and high-pass filters contribute to reducing noise in the MEG signal, making it easier to identify and analyze neural patterns.\n",
        "   - *Signal Enhancement:* By removing unwanted frequency components, filtering enhances the visibility of the desired neural activity, facilitating more accurate analysis and interpretation.\n",
        "\n",
        "**Sources:**\n",
        "1. ChatGBT.\n",
        "2. [Imaging.MRC-CBU: MEG PreProcessing](https://imaging.mrc-cbu.cam.ac.uk/meg/PreProcessing)\n",
        "3. van Driel, J., Olivers, C. N. L., Fahrenfort, J. J. (2021). \"High-pass filtering artifacts in multivariate classification of neural time series data.\", J. Neurosci. Methods, 352.ial.\n",
        "Conclusions"
      ],
      "id": "79bd7e89-25d9-4618-86f6-c762a20dab20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92485ffc-5d5d-48b8-b610-45f6755b9a63"
      },
      "source": [
        "# Let's look closer to choosen channel data"
      ],
      "id": "92485ffc-5d5d-48b8-b610-45f6755b9a63"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbe6181f-c77c-471e-8870-81643dea9add"
      },
      "source": [
        "The code below shows example how to draw a plot of MEG data as a function of time for the MEG0143 channel for the first 1200 samples."
      ],
      "id": "dbe6181f-c77c-471e-8870-81643dea9add"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dbc98ab-9e70-4682-9504-64d874e9bf37"
      },
      "outputs": [],
      "source": [
        "channel_name = \"MEG0143\"\n",
        "time = raw.times\n",
        "start_index= 0\n",
        "stop_index = 1200\n",
        "single_channel_signal_cropped = raw.get_data(channel_name, start = start_index, stop = stop_index)\n",
        "plt.plot(time[start_index:stop_index], single_channel_signal_cropped[0])\n",
        "plt.title(f'MEG Data for Channel {channel_name}')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.ylabel('Magnetic field [fT]')\n",
        "plt.show()"
      ],
      "id": "3dbc98ab-9e70-4682-9504-64d874e9bf37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2338e3ac-e0b2-43a1-ac5e-e1435776999f"
      },
      "source": [
        "At accoridng 0.3 [s] we see prominent peak. Let's look into the file sub-01_ses-meg_task-facerecognition_run-01_events.tsv what then happend. There are few first lines from the file:\n",
        "\n",
        "onset\tduration\tonset_sample\tstim_type\ttrigger\tstim_file  \n",
        "24.0473\t0\t26628\tUnfamiliar\t13\tmeg/u032.bmp  \n",
        "27.0473\t0\t29972\tUnfamiliar\t14\tmeg/u032.bmp  \n",
        "30.2545\t0\t33390\tUnfamiliar\t13\tmeg/u088.bmp  \n",
        "33.2618\t0\t36698\tUnfamiliar\t13\tmeg/u084.bmp  \n",
        "36.3536\t0\t40209\tFamous\t5\tmeg/f123.bmp  \n",
        "\n",
        "There no any matching event, but probably it is the phase of showing images as a testing stage of experiment.\n",
        "We see that the first trigger (showing an image of unfamiliar face) occured at 24.0473 let's look at this time range:\n"
      ],
      "id": "2338e3ac-e0b2-43a1-ac5e-e1435776999f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c608704e-7cb2-48c2-8afd-5465dc4a452e"
      },
      "outputs": [],
      "source": [
        "channel_name = \"MEG0143\"\n",
        "time = raw.times\n",
        "start_index= 26500\n",
        "stop_index = 27500\n",
        "single_channel_signal_cropped = raw.get_data(channel_name, start = start_index, stop = stop_index)\n",
        "plt.plot(time[start_index:stop_index], single_channel_signal_cropped[0])\n",
        "plt.title(f'MEG Data for Channel {channel_name}')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.ylabel('Magnetic field [fT]')\n",
        "plt.show()"
      ],
      "id": "c608704e-7cb2-48c2-8afd-5465dc4a452e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfaa50c8-0af5-4e43-85bd-854db05d7ca6"
      },
      "source": [
        "We also see response to trigger but it is quite noisy. That's why we move to the next step which will be averaging all the responses in order to improve signal to noise ratio."
      ],
      "id": "bfaa50c8-0af5-4e43-85bd-854db05d7ca6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0bdb858-e129-42cb-8ab9-9d6e0a751503"
      },
      "source": [
        "# Finding events in data, creating epochs of data"
      ],
      "id": "e0bdb858-e129-42cb-8ab9-9d6e0a751503"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "686e21d4-3c41-4b8f-b75f-f2e2cecd0058"
      },
      "source": [
        "As a first step, let's check what triggers/events were found in our data."
      ],
      "id": "686e21d4-3c41-4b8f-b75f-f2e2cecd0058"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ceaab40-a307-482f-a053-3c2f15c323f3"
      },
      "outputs": [],
      "source": [
        "events = mne.find_events(raw)"
      ],
      "id": "4ceaab40-a307-482f-a053-3c2f15c323f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3abb1ca8-8f7c-49d9-b1ae-c1ff817e6704"
      },
      "source": [
        "According to the README for our dataset, the events we are interested in are:\n",
        "\n",
        "| Trigger  | Label | Simplified Label |\n",
        "|----------|----------|----------|\n",
        "| 5 | Initial Famous Face  | FAMOUS |\n",
        "| 6 | Immediate Repeat Famous Face | FAMOUS |\n",
        "| 7 | Delayed Repeat Famous Face  | FAMOUS |\n",
        "| 13 | Initial Unfamiliar Face | UNFAMILIAR |\n",
        "| 14 | Immediate Repeat Famous Face | UNFAMILIAR |\n",
        "| 15 | Delayed Repeat Unfamiliar Face | UNFAMILIAR |\n",
        "| 17 | Initial Scrambled Face  | SCRAMBLED |\n",
        "| 18 | Immediate Repeat Scrambled Face | SCRAMBLED |\n",
        "| 19 | Delayed Repeat Scrambled Face | SCRAMBLED |\n",
        "\n",
        "\n",
        "So we exclude others.\n",
        "                       "
      ],
      "id": "3abb1ca8-8f7c-49d9-b1ae-c1ff817e6704"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "177f3333-7df8-4233-bbb3-adb6eb2e899f"
      },
      "outputs": [],
      "source": [
        "events = mne.pick_events(events, exclude=[ 256,  261,  262,  263,  269,\n",
        "  270,  271,  273,  274,  275, 4096, 4101, 4102, 4103, 4109, 4110, 4111, 4113, 4114,\n",
        " 4115, 4352])\n",
        "\n",
        "event_dict = {\n",
        "    \"Initial Famous Face\": 5,\n",
        "    \"Immediate Repeat Famous Face\": 6,\n",
        "    \"Delayed Repeat Famous Face\": 7,\n",
        "    \"Initial Unfamiliar Face\": 13,\n",
        "    \"Immediate Repeat Unfamiliar Face\": 14,\n",
        "    \"Delayed Repeat Unfamiliar Face\": 15,\n",
        "    \"Initial Scrambled Face\": 17,\n",
        "    \"Immediate Repeat Scrambled Face\": 18,\n",
        "    \"Delayed Repeat Scrambled Face\": 19}\n",
        "\n",
        "event_groups = {\n",
        "    \"famous\" : [\"Initial Famous Face\", \"Immediate Repeat Famous Face\", \"Delayed Repeat Famous Face\"],\n",
        "    \"unfamiliar\" : [\"Initial Unfamiliar Face\", \"Immediate Repeat Unfamiliar Face\", \"Delayed Repeat Unfamiliar Face\"],\n",
        "    \"scrambled\": [\"Initial Scrambled Face\", \"Immediate Repeat Scrambled Face\", \"Delayed Repeat Scrambled Face\"]\n",
        "}\n",
        "\n",
        "\n"
      ],
      "id": "177f3333-7df8-4233-bbb3-adb6eb2e899f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b7a5711-bdc6-4a78-847d-73b0f70fb17c"
      },
      "source": [
        "The code below is creating a plot of events in your MEG data, providing information about when certain events occurred during data acquisition. The resulting plot can be useful for understanding the temporal distribution of events in analysed experimental paradigm."
      ],
      "id": "3b7a5711-bdc6-4a78-847d-73b0f70fb17c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91c7b79b-56d3-403a-bcb5-fd58d3756fa2"
      },
      "outputs": [],
      "source": [
        "fig = mne.viz.plot_events(\n",
        "    events, sfreq=raw.info[\"sfreq\"], first_samp=raw.first_samp, event_id=event_dict\n",
        ")"
      ],
      "id": "91c7b79b-56d3-403a-bcb5-fd58d3756fa2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7394ca7-3420-4260-b841-7658fdc6574c"
      },
      "source": [
        "### Epochs"
      ],
      "id": "b7394ca7-3420-4260-b841-7658fdc6574c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23db7c03-0f19-41aa-b260-50276ebb5f00"
      },
      "source": [
        "In the context of neuroimaging, particularly in EEG (electroencephalography) and MEG (magnetoencephalography) studies, an epoch refers to a segment of continuous data centered around a specific event or marker. These events can be stimuli, responses, or other experimental triggers. The purpose of creating epochs is to segment the data into smaller, analysable time intervals to study neural activity related to particular events.\n",
        "\n"
      ],
      "id": "23db7c03-0f19-41aa-b260-50276ebb5f00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfa563b5-50be-46db-b163-e0c2923fca38"
      },
      "source": [
        "Code below create epochs from raw MEG data. It employs the mne.Epochs constructor, taking as input the raw MEG data (raw), event information (events), and additional parameters for epoch definition. The tmin parameter is set to -0.2 seconds, indicating the start time of each epoch relative to the onset of events, and tmax is set to 0.6 seconds, indicating the end time. The event_id parameter is provided with a dictionary (event_dict) that maps event labels to their corresponding event codes. The resulting epochs variable holds an Epochs object containing segmented time intervals (epochs) centered around specific events in the MEG data. By setting preload=True, the data is loaded into memory, facilitating faster access for subsequent analyses. These epochs are instrumental for studying and analyzing neural responses related to the specified events, such as computing event-related potentials (ERPs) or conducting further time-frequency analyses.\n"
      ],
      "id": "cfa563b5-50be-46db-b163-e0c2923fca38"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "634242cc-7ab3-4be2-b78f-62a5258ec8c7"
      },
      "outputs": [],
      "source": [
        "epochs = mne.Epochs(raw, events, tmin=-0.2, tmax=0.6, event_id=event_dict, preload=True)"
      ],
      "id": "634242cc-7ab3-4be2-b78f-62a5258ec8c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b42b02-c4f5-4323-bee1-94ec88cbbc50"
      },
      "source": [
        "In these two lines of code, an evoked response is computed by averaging the epochs, consolidating the neural activity across multiple trials related to specific events. The evoked.plot(picks='mag') command then generates a visualisation, specifically plotting the averaged magnetic field responses (picks='mag') to provide insight into the temporal dynamics and characteristics of the evoked neural activity recorded by magnetometers in the MEG data. This type of plot is commonly used to analyse and interpret event-related responses in magnetoencephalography experiments."
      ],
      "id": "42b42b02-c4f5-4323-bee1-94ec88cbbc50"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f91eaa0b-516a-4159-ae83-b96044cae7f5"
      },
      "outputs": [],
      "source": [
        "evoked = epochs.average()\n",
        "evoked.plot(picks='mag')"
      ],
      "id": "f91eaa0b-516a-4159-ae83-b96044cae7f5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c92016b-282a-47ee-b869-b215a0721490"
      },
      "source": [
        "After analysis of the images above, we can see that the largest peaks in the signal occur around 0.170 seconds, so let's draw ourselves a spatial map to look at where the signal was largest:"
      ],
      "id": "1c92016b-282a-47ee-b869-b215a0721490"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0152cad4-6844-4480-b8d3-6f409af16e1b"
      },
      "outputs": [],
      "source": [
        "fig = evoked.plot_topomap(\n",
        "    0.17, ch_type=\"mag\", show_names=True, colorbar=False, size=6, res=128\n",
        ")\n",
        "fig.suptitle(\"Visual response\")"
      ],
      "id": "0152cad4-6844-4480-b8d3-6f409af16e1b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f70c5fa9-23f5-4015-b3f1-2d7a694c821f"
      },
      "source": [
        "The intensity of colour corresponds to the magnitude of the signal at a specific time sample, with darker colors representing higher values at 170 ms. Dark red signifies positive values, while dark blue indicates negative values. Both are equally relevant, as we intend to analyse either their absolute values or their negation. Let's plot the average signal for chosen channels for example MEG1611 and MEG1631 (in these channels signal is strong, because they are placed in the dark red area)."
      ],
      "id": "f70c5fa9-23f5-4015-b3f1-2d7a694c821f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad55a750-127f-4e9c-824f-d8f1dbd8fba8"
      },
      "outputs": [],
      "source": [
        "epochs.plot_image(picks=[\"MEG1611\", \"MEG1631\"])"
      ],
      "id": "ad55a750-127f-4e9c-824f-d8f1dbd8fba8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0ac7347-a0e0-4fc6-85f5-29411f604703"
      },
      "source": [
        "The signal looks better on the channel and this is what we select for further analysis. Let's take a closer look at what the averaged signal looks like for the various triggers/events for this channel:"
      ],
      "id": "c0ac7347-a0e0-4fc6-85f5-29411f604703"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a19ea9e-d05b-4ad4-9945-88b7c7c35f98"
      },
      "outputs": [],
      "source": [
        "choosen_channel = \"MEG1631\"\n",
        "epochs[\"Initial Famous Face\"].plot_image(picks = choosen_channel)"
      ],
      "id": "8a19ea9e-d05b-4ad4-9945-88b7c7c35f98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b888eda-3bbe-490c-83ff-2ba2bc3de773"
      },
      "outputs": [],
      "source": [
        "epochs[\"Initial Unfamiliar Face\"].plot_image(picks = choosen_channel)"
      ],
      "id": "1b888eda-3bbe-490c-83ff-2ba2bc3de773"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a30a8161-557b-4fae-bcce-9482fe69224b"
      },
      "outputs": [],
      "source": [
        "epochs[\"Initial Scrambled Face\"].plot_image(picks = choosen_channel)"
      ],
      "id": "a30a8161-557b-4fae-bcce-9482fe69224b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41062f2d-50a9-4392-8051-452ef4646c61"
      },
      "source": [
        "Visually, they appear similar, but for the \"Initial Scrambled Face\" event, the peacock's value seems slightly diminished. Let's check this:\n"
      ],
      "id": "41062f2d-50a9-4392-8051-452ef4646c61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e74b8ca-c07c-4528-b8fa-1eeb13eae244"
      },
      "outputs": [],
      "source": [
        "event_labels = ['Initial Famous Face', 'Initial Unfamiliar Face', 'Initial Scrambled Face']\n",
        "channel_of_interest = \"MEG1631\"\n",
        "latency_magnitude_dict = {}\n",
        "time_range = (0.0, 0.2)\n",
        "\n",
        "for event_label in event_labels:\n",
        "    avg_epoch = epochs[event_label].average()   # Average the epochs for the specific event\n",
        "    data = avg_epoch.copy().pick([channel_of_interest]).get_data() # Pick the channel of interest and get the data for the specific event\n",
        "    data *= 1e15     # Multiply the entire signal by 10^15 to change the data into femtoteslas\n",
        "    time_points = avg_epoch.times     # Get the time points for the averaged epoch\n",
        "    start_idx, end_idx = np.where((time_points >= time_range[0]) & (time_points <= time_range[1]))[0][[0, -1]]  # Find the start and end indices for the specified time range\n",
        "    max_time_point = time_points[start_idx + np.argmax(np.abs(data[:, start_idx:end_idx + 1]))]\n",
        "    max_magnitude = np.max(np.abs(data[:, start_idx:end_idx + 1]))\n",
        "    latency = max_time_point - time_range[0] # Calculate latency as the difference between the time point and the start of the time range\n",
        "    latency_magnitude_dict[event_label] = {'latency': latency, 'magnitude': max_magnitude}\n",
        "\n",
        "for event_label, values in latency_magnitude_dict.items():\n",
        "    print(f'Event: {event_label}\\nLatency: {values[\"latency\"]:.3f} [s] \\nMagnitude: {values[\"magnitude\"]:.3f}[fT] \\n')\n"
      ],
      "id": "8e74b8ca-c07c-4528-b8fa-1eeb13eae244"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b782afa-4603-493a-af5d-92498c5247e8"
      },
      "source": [
        "# Tasks"
      ],
      "id": "6b782afa-4603-493a-af5d-92498c5247e8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3777668-c893-4b6b-8166-c478e4ef0b98"
      },
      "source": [
        "1. Plot the averaged signal for all channels in patient 1 for each trigger ['Initial Famous Face,' 'Initial Unfamiliar Face,' 'Initial Scrambled Face']. Compare this signal with that from the MEG1631 channel and explain why analysing only one channel or a group of several channels might be preferable over analysing the averaged signal from all channels.\n",
        "2. Perform the same tasks for the remaining four subjects and document the latency and amplitude of the peaks within the specified time range of 0-0.2 seconds, or 0-0.3 seconds if needed for an averaged signal from chosen channel. Channels can be selected manually through visual inspection or by choosing the one with the highest signal amplitude at a given time point.  Present the outcomes of latency and amplitude measurements in a tabular format and calculate the mean and standard deviation an generate a comparision plot.\n"
      ],
      "id": "a3777668-c893-4b6b-8166-c478e4ef0b98"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}